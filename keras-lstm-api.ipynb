{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First example\n",
    "\n",
    "What? LSTM feeding its output to dense layer without specified batch shape.\n",
    "\n",
    "* We only specify the `input_shape` (and not the full `batch_input_shape`)\n",
    "* We do not return the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_layer (LSTM)            (None, 30)                6120      \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 6,151\n",
      "Trainable params: 6,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Shape of feature tensor: (100, 10, 20)\n",
      "Shape of labels: (100, 1)\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.3873\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "n_timesteps = 10\n",
    "n_input_features = 20\n",
    "n_output_features= 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(n_timesteps, n_input_features), \n",
    "               units=n_output_features, \n",
    "               return_sequences=False, \n",
    "               name=\"lstm_layer\"))\n",
    "model.add(Dense(1, name=\"dense_layer\"))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "sample_size = 100\n",
    "sample_features = np.random.random(size=(sample_size, n_timesteps, n_input_features))\n",
    "sample_labels = np.random.randint(low=0, high=2, size=(sample_size, 1))\n",
    "\n",
    "print(\"Shape of feature tensor:\", sample_features.shape)\n",
    "print(\"Shape of labels:\", sample_labels.shape)\n",
    "\n",
    "_ = model.fit(x=sample_features, \n",
    "              y=sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First example (ct'd)\n",
    "\n",
    "What? Same as first example but the batch size is explicitly specified.\n",
    "\n",
    "* Specifiy `batch_input_shape` (and not just `input_shape`)\n",
    "* Do not return the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_layer (LSTM)            (5, 30)                   6120      \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (5, 1)                    31        \n",
      "=================================================================\n",
      "Total params: 6,151\n",
      "Trainable params: 6,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Shape of feature tensor: (50, 10, 20)\n",
      "Shape of labels: (50, 1)\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3051\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "n_timesteps = 10\n",
    "n_input_features = 20\n",
    "n_output_features= 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(batch_input_shape=(batch_size, n_timesteps, n_input_features), \n",
    "               units=n_output_features, \n",
    "               return_sequences=False, \n",
    "               name=\"lstm_layer\"))\n",
    "model.add(Dense(1, name=\"dense_layer\"))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "sample_size = 10 * batch_size # If `sample_size % batch_size != 0` then error!\n",
    "sample_features = np.random.random(size=(sample_size, n_timesteps, n_input_features))\n",
    "sample_labels = np.random.randint(low=0, high=2, size=(sample_size, 1))\n",
    "\n",
    "print(\"Shape of feature tensor:\", sample_features.shape)\n",
    "print(\"Shape of labels:\", sample_labels.shape)\n",
    "\n",
    "_ = model.fit(x=sample_features, \n",
    "              y=sample_labels, \n",
    "              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second example\n",
    "\n",
    "What? LSTM returning the output sequences and not only the last output sequence. Thus output has first to be further processed in another layer.\n",
    "\n",
    "* We set `return_sequences=True` and now we return tensors of shape `(n_timesteps, n_output_features)` from the LSTM layer. This cannot processed as is by our `Dense` layer. In this example we add a `Flatten` layer that flattens the LSTM output to a vector that can be feed to the `Dense` layer. (Another option is to add an another LSTM layer instead of the `Flatten` layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_layer (LSTM)            (None, 10, 30)            6120      \n",
      "_________________________________________________________________\n",
      "flatten_layer (Flatten)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 6,421\n",
      "Trainable params: 6,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Shape of feature tensor: (100, 10, 20)\n",
      "Shape of labels: (100, 1)\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.4770\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "n_timesteps = 10\n",
    "n_input_features = 20\n",
    "n_output_features= 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(n_timesteps, n_input_features), \n",
    "               units=n_output_features, \n",
    "               return_sequences=True,\n",
    "               name=\"lstm_layer\"))\n",
    "model.add(Flatten(name=\"flatten_layer\"))\n",
    "model.add(Dense(1, name=\"dense_layer\"))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "sample_size = 100\n",
    "sample_features = np.random.random(size=(sample_size, n_timesteps, n_input_features))\n",
    "sample_labels = np.random.randint(low=0, high=2, size=(sample_size, 1))\n",
    "\n",
    "print(\"Shape of feature tensor:\", sample_features.shape)\n",
    "print(\"Shape of labels:\", sample_labels.shape)\n",
    "\n",
    "_ = model.fit(x=sample_features, \n",
    "              y=sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second example (ct'd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_layer (LSTM)            (5, 10, 30)               6120      \n",
      "_________________________________________________________________\n",
      "flatten_layer (Flatten)      (5, 300)                  0         \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (5, 1)                    301       \n",
      "=================================================================\n",
      "Total params: 6,421\n",
      "Trainable params: 6,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Shape of feature tensor: (50, 10, 20)\n",
      "Shape of labels: (50, 1)\n",
      "Epoch 1/1\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2682\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "n_timesteps = 10\n",
    "n_input_features = 20\n",
    "n_output_features= 30\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(batch_input_shape=(batch_size, n_timesteps, n_input_features), \n",
    "               units=n_output_features, \n",
    "               return_sequences=True,\n",
    "               name=\"lstm_layer\"))\n",
    "model.add(Flatten(name=\"flatten_layer\"))\n",
    "model.add(Dense(1, name=\"dense_layer\"))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "sample_size = 10 * batch_size\n",
    "sample_features = np.random.random(size=(sample_size, n_timesteps, n_input_features))\n",
    "sample_labels = np.random.randint(low=0, high=2, size=(sample_size, 1))\n",
    "\n",
    "print(\"Shape of feature tensor:\", sample_features.shape)\n",
    "print(\"Shape of labels:\", sample_labels.shape)\n",
    "\n",
    "_ = model.fit(x=sample_features, \n",
    "              y=sample_labels,\n",
    "              batch_size=batch_size) # Note: If `batch_size` is not specified here --> error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above note `batch_size` defaults to 32 in `fit` if not given."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
